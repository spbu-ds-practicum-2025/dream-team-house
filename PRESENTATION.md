# Презентация промежуточного демо
## Что конкретно сделано и с какими проблемами столкнулись

---

## Слайд 1: Что реально реализовано

### На слайде:
**Заголовок:** Что мы реализовали за время работы

**Конкретные результаты:**

**Text Service (Python/FastAPI):**
- ✅ 3 базы данных PostgreSQL с разными схемами (documents, edits, token_budget)
- ✅ Реализовали репликацию между 3 узлами - когда один узел получает правку, он отправляет её остальным двум через POST запросы
- ✅ Сделали механизм catch-up для восстановления узлов - если узел был выключен, при старте он запрашивает у соседей все пропущенные правки
- ✅ Добавили контроль бюджета токенов - когда лимит исчерпан, сервис возвращает HTTP 429

**Chat Service (Python/FastAPI):**
- ✅ Интеграция с Redis Streams - используем XADD для добавления сообщений и XRANGE для чтения
- ✅ Ограничение в 1000 сообщений через параметр MAXLEN при добавлении
- ✅ Структурированные типы сообщений (EditIntent, EditComment, EditOperation)

**AI Agent (Node.js):**
- ✅ Полный цикл агента: читает чат → читает документ → генерирует намерение → подтверждает → создаёт правку → отправляет
- ✅ Интеграция с OpenAI через ProxyAPI (https://api.proxyapi.ru/openai/v1)
- ✅ Обработка ошибок с экспоненциальным backoff - если запрос упал, агент ждёт 1с, потом 2с, потом 4с, потом 8с
- ✅ Обработка 429 ошибки - агент останавливается, если бюджет кончился

**Analytics Service (Python/FastAPI):**
- ✅ Сохранение событий в PostgreSQL с timestamp
- ✅ Агрегация метрик за 1 час, 24 часа и 7 дней
- ✅ Подсчёт токенов, количества правок, активных агентов

**Frontend (Next.js 15):**
- ✅ 4 страницы: главная, документ, чат, аналитика
- ✅ Auto-refresh каждые 2-3 секунды через setInterval
- ✅ Responsive дизайн на Tailwind CSS

### Что говорить:
"Расскажу конкретно, что мы сделали.

По Text Service - это самый сложный компонент. Мы поднимаем три отдельные базы PostgreSQL, и когда один узел получает правку, он должен отправить её на два других. Пришлось реализовать механизм catch-up - если какой-то узел был выключен, при старте он запрашивает у соседей все правки, которые пропустил. Плюс добавили контроль бюджета - когда токены кончаются, сервис начинает возвращать 429 ошибку.

Chat Service работает на Redis Streams. Тут главная фишка - это структура данных Redis. Мы используем XADD для добавления сообщения и автоматически ограничиваем поток 1000 сообщениями через MAXLEN. XRANGE позволяет читать диапазон сообщений по ID.

AI Agent - самая интересная часть. Агент проходит через несколько фаз: читает все сообщения из чата, получает текущий документ, генерирует намерение, потом подтверждает его после повторной проверки, создаёт конкретную операцию и только потом отправляет. Мы добавили retry logic с экспоненциальной задержкой - если OpenAI не ответил, агент ждёт 1 секунду, потом 2, потом 4, и так до 8 секунд.

Analytics просто пишет все события в базу с timestamp-ами, а потом агрегирует их по периодам.

Frontend обновляется каждые 2-3 секунды автоматически - мы используем setInterval для polling новых данных с бэкенда."

---

## Слайд 2: Конкретные технические проблемы и решения

### На слайде:
**Заголовок:** С чем реально столкнулись

**Проблема 1: Неправильный расчёт API вызовов**
- Думали: 1 правка = 1 запрос к OpenAI
- Обнаружили: 1 правка = 4 запроса (intent → confirm → generate → review)
- Было: 3 агента × 2 правки × 4 вызова = 24 запроса (перерасход в 2.4 раза!)
- Сделали: 2 агента × 1 правка × 4 вызова = 8 запросов (уложились в лимит 10)

**Проблема 2: Конкурентные правки**
- Симптом: Два агента пытаются править одно место → один затирает изменения другого
- Пробовали: Оптимистичные блокировки через version field
- Не сработало: Race condition между SELECT и UPDATE
- Решили: PostgreSQL транзакции с уровнем изоляции READ COMMITTED + retry на 5 попыток с задержкой 1-2-4-8-16 секунд

**Проблема 3: Anchor не найден**
- Симптом: Агент ищет текст "роботы были счастливы", а в документе уже "роботы стали счастливыми"
- Случается: Когда другой агент изменил текст после того, как первый прочитал документ
- Решили: Перед применением правки агент повторно читает документ, если anchor не найден - отменяет операцию и пишет в чат комментарий

**Проблема 4: Репликация при падении узла**
- Симптом: Узел А был выключен 2 часа, потерял 50 правок
- Решили: При старте узел делает GET /api/edits?after_version=X к обоим соседям, получает список пропущенных правок и применяет их последовательно

### Что говорить:
"Теперь про реальные проблемы, с которыми столкнулись.

Первая и самая неожиданная - мы неправильно посчитали бюджет на OpenAI. Смотрели на протокол и думали, что одна правка - это один запрос. А когда начали разбираться в файле `multi_agent_editor_demo_Version2.py`, оказалось, что протокол требует 4 запроса на каждую правку: сначала агент генерирует намерение, потом подтверждает его, потом создаёт операцию, и в конце делает финальное ревью. Получалось 3 агента по 2 правки - это 24 запроса вместо 6. Перерасход больше чем в 2 раза! Пришлось срочно снижать: оставили 2 агента по 1 правке.

Вторая проблема - конкурентность. Когда два агента одновременно правят документ, возникает race condition. Мы сначала пытались делать оптимистичные блокировки через version field - читаешь документ версии 5, правишь, отправляешь с version=5. Если в базе уже версия 6 - откат. Но между SELECT и UPDATE всё равно была гонка. В итоге решили через транзакции PostgreSQL с уровнем изоляции READ COMMITTED. Если правка не прошла - агент делает retry с экспоненциальным backoff: 1 секунда, 2, 4, 8, 16. До 5 попыток.

Третья - anchor-based операции. Агент читает документ, видит текст 'роботы были счастливы', решает вставить что-то после этой фразы. Но пока он думал, другой агент уже изменил эту фразу на 'роботы стали счастливыми'. Первый агент отправляет правку, но anchor не находится. Мы добавили проверку - перед применением агент повторно читает документ, если anchor не найден - отменяет операцию и пишет в чат, что передумал.

Четвёртая - репликация при падении узла. Если один узел Text Service был выключен, он пропускает все правки. Мы реализовали catch-up механизм: при старте узел спрашивает соседей 'дайте мне все правки после версии X', получает массив и применяет их по порядку. Тестировали - узел был выключен 2 часа, вернулся, подтянул 50 правок за минуту."

---

## Слайд 3: Сложности с deployment и инфраструктурой

### На слайде:
**Заголовок:** Проблемы с автоматизацией развёртывания

**Проблема 1: Docker не установлен на сервере**
- Задача: CI/CD должен работать на чистом сервере
- Решение: Добавили в GitHub Actions скрипт проверки и установки Docker + Docker Compose
```bash
if ! command -v docker &> /dev/null; then
    curl -fsSL https://get.docker.com -o get-docker.sh
    sh get-docker.sh
fi
```

**Проблема 2: SSL сертификаты Let's Encrypt**
- Задача: 6 доменов, нужны автоматические сертификаты
- Пробовали: certbot вручную
- Проблема: certbot требует остановки сервисов на порту 80, ломает работу
- Решили: Traefik с автоматическим ACME, получает сертификаты без остановки сервисов

**Проблема 3: Переменные окружения**
- Задача: OPENAI_API_KEY и другие секреты нельзя хранить в коде
- Решение: GitHub Secrets → SSH → создание .env файла на сервере
- Подводный камень: Кавычки в переменных ломали docker-compose, пришлось экранировать

**Проблема 4: Порядок запуска сервисов**
- Симптом: AI Agent стартует раньше Text Service → connection refused
- Пробовали: depends_on в docker-compose
- Не помогло: depends_on ждёт только старт контейнера, не готовность сервиса
- Решили: Добавили в агента retry при подключении + healthcheck в Text Service

**Проблема 5: Объём логов**
- Симптом: После суток работы сервер забился логами на 5GB
- Решили: Настроили log rotation в docker-compose (max-size: 10m, max-file: 3)

### Что говорить:
"Deployment оказался неожиданно геморным.

Первое - сервер был чистый, без Docker. GitHub Actions должен сам всё поставить. Добавили скрипт, который проверяет наличие Docker, и если его нет - качает установочный скрипт с get.docker.com и выполняет. Тут проблема была с правами - нужно добавлять пользователя в группу docker, иначе приходится всё через sudo.

Второе - SSL сертификаты. У нас 6 доменов, и для каждого нужен сертификат от Let's Encrypt. Сначала хотели использовать certbot, но он требует остановить все сервисы на 80 порту для прохождения ACME challenge. Это значит downtime при каждом обновлении сертификата. В итоге перешли на Traefik - он работает как reverse proxy и автоматически получает сертификаты в фоне, без остановки сервисов. Просто добавляешь labels в docker-compose, и Traefik всё делает сам.

Третье - секреты. OPENAI_API_KEY и пароли к базам нельзя коммитить в код. Используем GitHub Secrets, но их нужно прокинуть на сервер и создать .env файл. Тут была ловушка с кавычками - если в переменной есть пробелы или спецсимволы, docker-compose их неправильно парсит. Пришлось экранировать через printf вместо echo.

Четвёртое - порядок запуска. AI Agent запускается и сразу пытается подключиться к Text Service, но тот ещё не готов. depends_on не помогает, потому что он ждёт только старт контейнера, а не готовность приложения. Добавили в агента retry logic - если не подключился, ждёт 2 секунды и пробует снова, до 10 попыток. Плюс в Text Service добавили healthcheck, чтобы можно было точно определить готовность.

Пятое - логи. Запустили систему, через сутки сервер упал - кончилось место на диске. Оказалось, агенты очень болтливые, логов накопилось 5GB. Настроили в docker-compose log rotation: максимум 10MB на файл, максимум 3 файла. Старые автоматически удаляются."

---

## Слайд 4: Сложности с тестированием и отладкой

### На слайде:
**Заголовок:** Что было сложно протестировать

**Проблема 1: Тестирование репликации**
- Задача: Проверить, что узел восстанавливается после падения
- Сложность: Нужно имитировать выключение узла посреди работы
- Решение: 
  - Unit test: мокируем HTTP запросы к соседям
  - Integration test: поднимаем 3 контейнера, делаем `docker stop text-service-b`, ждём 30 секунд, делаем правки, потом `docker start text-service-b` и проверяем, что он подтянул изменения

**Проблема 2: Race conditions**
- Задача: Воспроизвести ситуацию, когда 2 агента одновременно правят
- Сложность: Агенты работают асинхронно, нельзя предсказать таймінг
- Решение: Написали скрипт, который запускает 10 агентов одновременно с минимальной задержкой (0-100ms), и проверяем, что ни одна правка не потерялась

**Проблема 3: Тестирование OpenAI API**
- Задача: Протестировать агента без реальных запросов (они дорогие)
- Решение: Создали mock OpenAI API сервер на Express.js, который возвращает заранее подготовленные JSON ответы
- Подводный камень: Реальный API иногда возвращает невалидный JSON или неожиданные структуры, пришлось добавлять fallback

**Проблема 4: E2E тест полного цикла**
- Задача: Создать документ → запустить агента → проверить, что появилась правка
- Сложность: Агент может занять 30-60 секунд на полный цикл (4 запроса к OpenAI)
- Решение: E2E тест в CI ждёт до 2 минут с проверкой каждые 5 секунд

**Проблема 5: Отладка в production**
- Симптом: На локальной машине всё работает, в production падает
- Причина 1: Разные версии PostgreSQL (локально 15, на сервере 14) → разный синтаксис
- Причина 2: Network latency - локально 1ms, между сервисами 50-100ms → таймауты
- Решение: Добавили подробное логирование с timestamp и request_id для трейсинга

### Что говорить:
"Тестирование распределённой системы - отдельный челлендж.

Первое - как проверить репликацію? В unit тестах мы мокируем HTTP запросы к соседним узлам. Но в integration тестах нужно реально поднимать три контейнера PostgreSQL. Мы делаем так: поднимаем три узла, выключаем один через `docker stop`, делаем несколько правок в оставшиеся узлы, ждём 30 секунд, включаем упавший узел и проверяем, что он подтянул все изменения. Это долго, один такой тест идёт минуту.

Второе - race conditions. Чтобы воспроизвести ситуацию, когда два агента одновременно пытаются править, написали скрипт, который запускает 10 агентов с рандомной задержкой 0-100 миллисекунд. Они все начинают работать почти одновременно, и мы проверяем, что ни одна правка не потерялась и все прошли через БД транзакционно.

Третье - OpenAI API. Реальные запросы дорогие, нельзя их делать в каждом тесте. Подняли mock сервер на Express, который возвращает заранее подготовленные JSON ответы. Но столкнулись с тем, что реальный API иногда возвращает невалидный JSON или неожиданные поля. Пришлось добавлять в агента fallback логику на случай неожиданного формата.

Четвёртое - E2E тесты. Полный цикл агента - это 4 запроса к OpenAI, каждый занимает 5-10 секунд. Итого до минуты на один цикл. В CI мы запускаем E2E тест, который ждёт до 2 минут с проверкой каждые 5 секунд - появилась ли правка.

Пятое - production отладка. Локально всё работает, на сервере падает. Первая причина - разные версии PostgreSQL, на сервере стояла 14 версия, у нас локально 15, и некоторые SQL запросы работали по-разному. Вторая - network latency. Локально между сервисами 1 миллисекунда, на сервере 50-100. Таймауты, которые локально работали, на сервере истекали. Добавили везде подробное логирование с timestamp и request_id, чтобы можно было трейсить запрос через все сервисы."

---

## Слайд 5: Результаты запуска демо и что получилось

### На слайде:
**Заголовок:** Запуск multi_agent_editor_demo_Version2.py

**Эксперимент:**
- Запустили 10 агентов
- Начальный текст: "Документ начинается здесь."
- Тема: научно-фантастический детский рассказ про роботов
- Результат: 31 версия документа

**Версия 1 (начальная):**
```
Документ начинается здесь. В далёком XXIII веке, когда над 
городами тихо проплывали орбитальные станции, а по ночам в 
окнах вместо звёзд мерцали огни ремонтных дронов, детям всё 
равно было немного страшно в темноте. Роботы-няни читали им 
сказки про космос, следили за кошмарами в снах и аккуратно 
приглушали свет, но сами тайком задавали друг другу один и 
тот же вопрос: что значит быть «хорошим человеком»?
```

**Версия 17 (промежуточная):**
```
В двадцать третьем веке города светились мягким неоновым 
сиянием, по небу тихо скользили дроны, а в парках вместо 
фонтанов шептали солнечные деревья. В таких городах жили 
не только люди, но и роботы — умные, вежливые и очень 
любопытные. Больше всего на свете они хотели понять, 
как это — быть человеком...
```

**Версия 31 (финальная):**
Полноценный рассказ на несколько абзацев с развитым сюжетом

**Что видно:**
- Агенты переписали начало, убрав заглушку "Документ начинается здесь"
- Улучшили описания (добавили "мягким неоновым сиянием", "солнечные деревья")
- Развили сюжет про роботов, которые хотят понять людей
- Каждый агент вносил локальные правки, не ломая общую структуру

### Что говорить:
"Перед тем как делать систему, я запустил демо-файл, чтобы понять, как это всё должно работать.

Взял файл `multi_agent_editor_demo_Version2.py` из корня проекта, запустил с 10 агентами. Дал им начальную затравку - просто 'Документ начинается здесь' и один абзац про роботов в будущем. Указал тему - научно-фантастический детский рассказ про роботов, которые хотят стать людьми с моралью.

Агенты начали работать параллельно. За время работы получилось 31 версия документа.

В первой версии был этот базовый текст - заглушка 'Документ начинается здесь' и абзац про XXIII век с роботами-нянями, которые спрашивают себя, что значит быть хорошим человеком.

К версии 17 уже было видно серьёзные изменения. Агенты убрали заглушку, переписали начало более художественно - вместо 'далёкий век' написали 'города светились мягким неоновым сиянием', добавили деталей типа 'солнечные деревья вместо фонтанов'. Текст стал более живым.

К версии 31 получился полноценный рассказ на несколько абзацев с развитым сюжетом про роботов.

Что важно - каждый агент делал локальные правки. Один добавлял описания, другой улучшал диалоги, третий развивал сюжет. Но они не конфликтовали между собой, потому что работают через протокол с намерениями. Прежде чем править, агент пишет в чат 'я хочу добавить описание парка', другие видят это сообщение и не лезут туда же.

Это и показало нам, что протокол реально работает - 10 агентов параллельно создали связный текст, а не кашу."

---

## Инструкции для демонстрации

### Перед презентацией:
1. Открыть в браузере:
   - Главную страницу frontend
   - Страницу документа
   - Страницу чата
   - Страницу аналитики

2. Подготовить скриншоты:
   - Архитектурная схема (из README или нарисовать)
   - Страницы frontend (сделать 4 скриншота)
   - Графики из аналитики
   - Пример версий документа (версия 1 → 17 → 31)

3. Проверить, что всё работает:
   ```bash
   docker-compose up -d
   # Подождать 30 секунд
   curl http://localhost/api/document/current
   ```

### Во время демо:
- **Слайд 1**: Показать общую схему, объяснить идею
- **Слайд 2**: Показать пример трансформации текста (версии 1, 17, 31)
- **Слайд 3**: Объяснить архитектуру на схеме
- **Слайд 4**: Честно рассказать про проблемы
- **Слайд 5**: 
  - Открыть live сайт
  - Показать создание документа
  - Показать real-time обновления
  - Показать аналитику

### Ключевые месседжи:
1. "Мы реализовали полноценную распределённую систему, не макет"
2. "Протокол с намерениями предотвращает конфликты"
3. "Все свойства из курса: replication, concurrency, safety, liveness"
4. "Столкнулись с реальными проблемами распределённых систем и решили их"
5. "Система работает в продакшене с автоматическим deployment"

---

## Дополнительные материалы для вопросов

**Q: Почему именно такой протокол с 4 фазами?**
A: Чтобы агенты не конфликтовали. Если агент видит, что кто-то уже сделал похожее — он может отменить своё намерение на любой фазе.

**Q: Как обрабатываются конфликты?**
A: PostgreSQL транзакции + last-write-wins по timestamp. Проигравший откатывается и пробует снова с новой версией документа.

**Q: Что если узел упадёт?**
A: Load balancer переключит трафик на живые узлы. При восстановлении узел подтянет пропущенные правки через catch-up механизм.

**Q: Сколько стоит запуск?**
A: ~$0.01-0.02 за сессию с 2 агентами и 1 правкой на агента (8 API вызовов к OpenAI).

**Q: Можно масштабировать?**
A: Да, агенты stateless — можно запустить хоть 100. Узлы Text Service тоже можно добавлять. Bottleneck только в OpenAI API rate limits.
